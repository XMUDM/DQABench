from __future__ import annotations

## When running alone, you need to add
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

import re
import warnings
from typing import Dict

from langchain.callbacks.manager import (
    AsyncCallbackManagerForChainRun,
    CallbackManagerForChainRun,
)
from langchain.chains.base import Chain
from langchain.chains.llm import LLMChain
from langchain.pydantic_v1 import Extra, root_validator
from langchain.schema import BasePromptTemplate
from langchain.schema.language_model import BaseLanguageModel
import requests
from typing import List, Any, Optional
from datetime import datetime
from langchain.prompts import PromptTemplate
from server.agent import model_container
import json
from server.agent.tools.database.Postgre import PG
from pydantic import BaseModel, Field

# _PROMPT_TEMPLATE = """
# To assess this question, you need to know whether it involves tables, columns (name, data type or cardinality), or keys (primary or foreign). Then, specify the objectives you need information about (the fewer, the better), and the tool I provide will return the corresponding details.

# Your response should follow the format below. Please note that all marks like ```text must be included, as they are used to extract the answers.

# ```text
# ${{Information you need to know, which can be tables, columns, cardinality, keys}}


# ```output (not generated by you)
# ${{Specific information returned by the tool}}


# ```answer
# ${{Your answer}}


# Now, My question is:
# question: {question}
# """

# PROMPT = PromptTemplate(
#     input_variables=["question"],
#     template=_PROMPT_TEMPLATE,
# )

def database_info(query):
    try:
        table_info = ""
        column_info = [] 
        cardinity_info = ""
        try:
            database = PG()
        except Exception as e:
            return "Database connection failed, please check if there is any connection problem"
        if "table" in query or "表" in query:
            table_info = database.get_tables_info("public")
        if "column" in query or "列" in query:
            table_info = database.get_tables_info("public")
            for table in table_info:
                column_info.append(str(table) + ":" + str(database.get_columns_info(table,"public")))
        if "key" in query or "键" in query:
            cardinity_info = database.get_key("public")
        database.close()
        return str(table_info) + '\n' + str(column_info) + "\n" + str(cardinity_info)
    except Exception as e:
        return str(e) + "Failed to obtain database information. Please make sure that the action input contains table, column or key."

# class LLMDBinfoChain(Chain):
#     llm_chain: LLMChain
#     llm: Optional[BaseLanguageModel] = None
#     """[Deprecated] LLM wrapper to use."""
#     prompt: BasePromptTemplate = PROMPT
#     """[Deprecated] Prompt to use to translate to python if necessary."""
#     input_key: str = "question"  #: :meta private:
#     output_key: str = "answer"  #: :meta private:

#     class Config:
#         """Configuration for this pydantic object."""

#         extra = Extra.forbid
#         arbitrary_types_allowed = True

#     @root_validator(pre=True)
#     def raise_deprecation(cls, values: Dict) -> Dict:
#         if "llm" in values:
#             warnings.warn(
#                 "Directly instantiating an LLMDBinfoChain with an llm is deprecated. "
#                 "Please instantiate with llm_chain argument or using the from_llm "
#                 "class method."
#             )
#             if "llm_chain" not in values and values["llm"] is not None:
#                 prompt = values.get("prompt", PROMPT)
#                 values["llm_chain"] = LLMChain(llm=values["llm"], prompt=prompt)
#         return values

#     @property
#     def input_keys(self) -> List[str]:
#         """Expect input key.

#         :meta private:
#         """
#         return [self.input_key]

#     @property
#     def output_keys(self) -> List[str]:
#         """Expect output key.

#         :meta private:
#         """
#         return [self.output_key]

#     def _evaluate_expression(self, expression: str) -> str:
#         try:
#             output = database_info(expression)
#         except Exception as e:
#             output = "The information you entered is incorrect, please try again"
#         return output

#     def _process_llm_result(
#             self, llm_output: str, run_manager: CallbackManagerForChainRun
#     ) -> Dict[str, str]:

#         run_manager.on_text(llm_output, color="green", verbose=self.verbose)
#         llm_output = llm_output.strip()
#         text_match = re.search(r"^```text(.*?)", llm_output, re.DOTALL)
#         if text_match:
#             expression = text_match.group(1)
#             output = self._evaluate_expression(expression)
#             run_manager.on_text("\nAnswer: ", verbose=self.verbose)
#             run_manager.on_text(output, color="yellow", verbose=self.verbose)
#             answer = "Answer: " + output
#         else:
#             return {self.output_key: f"unknown format from LLM: {llm_output}"}
#         return {self.output_key: answer}

#     async def _aprocess_llm_result(
#             self,
#             llm_output: str,
#             run_manager: AsyncCallbackManagerForChainRun,
#     ) -> Dict[str, str]:
#         await run_manager.on_text(llm_output, color="green", verbose=self.verbose)
#         llm_output = llm_output.strip()
#         text_match = re.search(r"^```text(.*?)", llm_output, re.DOTALL)

#         if text_match:
#             expression = text_match.group(1)
#             output = self._evaluate_expression(expression)
#             await run_manager.on_text("\nAnswer: ", verbose=self.verbose)
#             await run_manager.on_text(output, color="yellow", verbose=self.verbose)
#             answer = "Answer: " + output
#         elif llm_output.startswith("Answer:"):
#             answer = llm_output
#         elif "Answer:" in llm_output:
#             answer = "Answer: " + llm_output.split("Answer:")[-1]
#         else:
#             raise ValueError(f"unknown format from LLM: {llm_output}")
#         return {self.output_key: answer}

#     def _call(
#             self,
#             inputs: Dict[str, str],
#             run_manager: Optional[CallbackManagerForChainRun] = None,
#     ) -> Dict[str, str]:
#         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
#         print("print3"+ str(inputs))
#         _run_manager.on_text(inputs[self.input_key])
#         print("print2"+inputs[self.input_key])
#         llm_output = self.llm_chain.predict(
#             question=inputs[self.input_key],
#             stop=["```output"],
#             callbacks=_run_manager.get_child(),
#         )
#         return self._process_llm_result(llm_output, _run_manager)

#     async def _acall(
#             self,
#             inputs: Dict[str, str],
#             run_manager: Optional[AsyncCallbackManagerForChainRun] = None,
#     ) -> Dict[str, str]:
#         _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()
#         await _run_manager.on_text(inputs[self.input_key])
#         llm_output = await self.llm_chain.apredict(
#             question=inputs[self.input_key],
#             stop=["```output"],
#             callbacks=_run_manager.get_child(),
#         )
#         return await self._aprocess_llm_result(llm_output, _run_manager)

#     @property
#     def _chain_type(self) -> str:
#         return "llm_DBinfo_chain"

#     @classmethod
#     def from_llm(
#             cls,
#             llm: BaseLanguageModel,
#             prompt: BasePromptTemplate = PROMPT,
#             **kwargs: Any,
#     ) -> LLMDBinfoChain:
#         llm_chain = LLMChain(llm=llm, prompt=prompt)
#         return cls(llm_chain=llm_chain, **kwargs)

def database_structure_info(query: str):
    # model = model_container.MODEL
    # llmchain = LLMDBinfoChain.from_llm(model, verbose=True, prompt=PROMPT)
    # ans = llmchain.run(query)
    res = database_info(query)
    ans = {"text": res}
    return ans

class DBStructureInput(BaseModel):
    query: str = Field(description="the database structure information you need to know, must one or more in [tables, columns, keys]")

if __name__ == "__main__":
    database_info("tables columns keys")