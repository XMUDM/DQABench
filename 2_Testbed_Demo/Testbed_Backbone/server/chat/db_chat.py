from fastapi import Body
from fastapi.responses import StreamingResponse
from server.agent.tools_select import tools, tool_names
from server.utils import wrap_done, get_ChatOpenAI
from server.agent.callbacks import CustomAsyncIteratorCallbackHandler, Status
from langchain.chains import LLMChain
from langchain.callbacks import AsyncIteratorCallbackHandler
from langchain.memory import ConversationBufferWindowMemory
from typing import AsyncIterable
from langchain.agents import AgentExecutor, LLMSingleActionAgent
from configs import LLM_MODELS, TEMPERATURE, HISTORY_LEN
from langchain.agents import AgentType, Tool, initialize_agent
from server.agent.custom_template import CustomOutputParser, CustomToolPromptTemplate, CustomKnowledgePromptTemplate
import asyncio
import re
import json
from langchain.prompts.chat import ChatPromptTemplate
from typing import List
from server.chat.utils import History
from server.utils import get_prompt_template
from configs import prompt_config
from langchain.chains.router import MultiPromptChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.prompts import PromptTemplate
from server.agent import model_container
from server.knowledge_base.kb_service.base import get_kb_details

from server.utils import BaseResponse, get_prompt_template
from typing import AsyncIterable, List, Optional
from server.knowledge_base.kb_service.base import KBService, KBServiceFactory
import os
from urllib.parse import urlencode
from server.knowledge_base.kb_doc_api import search_docs

import importlib

importlib.reload(prompt_config)
prompts = prompt_config.DB_PROMPT_TEMPLATE

prompt_infos = [
    {
        "name": "general",
        "description": "Good for answering general database issues. Bad for answering issues about opengauss, gaussDB, Opengauss, gaussDB and specific database instance.",
        "prompt_template": prompts["general"],
        "tools": None
    },
    {
        "name": "gauss",
        "description": "Good for answering database issues, which are related to opengauss, gaussDB, Opengauss and gaussDB databases.",
        "prompt_template": prompts["gauss"],
        "tools": None
    },
    {
        "name": "management",
        "description": "Good for answering management questions when mentioning the specific database instance \"OurDB\", which involve specific tables, columns, workloads, states, etc.",
        "prompt_template": prompts["management"],
        "tools": tools
    },
    {
        "name": "analysis",
        "description": "Good for answering data analysis questions when mentioning the specific database instance \"OurDB\", which use SQL queries to analyze the data.",
        "prompt_template": prompts["analysis"],
        "tools": tools
    },
    {
        "name": "normal",
        "description": "Good for answering issues unrelated to database field.",
        "prompt_template": prompts["normal"],
        "tools": None
    }
]


async def db_chat(query: str = Body(..., description="User Input", examples=["恼羞成怒"]),
                  history: List[History] = Body([],
                                                description="Historical Dialogue",
                                                examples=[[
                                                    {"role": "user", "content": "我们来玩成语接龙，我先来，生龙活虎"},
                                                    {"role": "assistant", "content": "虎头虎脑"}]]
                                                ),
                  stream: bool = Body(False, description="Streaming Output"),
                  model_name: str = Body(LLM_MODELS[0], description="LLM name."),
                  temperature: float = Body(TEMPERATURE, description="LLM sampling temperature", ge=0.0, le=1.0),
                  max_tokens: int = Body(None, description="Limit the number of tokens generated by LLM. The default value is None, which represents the maximum value of the model."),
                  # top_p: float = Body(TOP_P, description="LLM core sampling. Do not set this at the same time as temperature.", gt=0.0, lt=1.0),
                  prompt_name: str = Body("default",
                                          description="The name of the prompt template to use (configured in configs-prompt_config.py)"),
                  ):
    history = [History.from_data(h) for h in history]

    knowledge_base_name = "OpenGauss"
    top_k = 3
    score_threshold = 1

    kb = KBServiceFactory.get_service_by_name(knowledge_base_name)
    if kb is None:
        return BaseResponse(code=404, msg=f"Knowledge base {knowledge_base_name} not found")

    async def db_chat_iterator(query: str,
                               history: List[History] = [],
                               model_name: str = LLM_MODELS[0],
                               prompt_name: str = prompt_name,
                               ) -> AsyncIterable[str]:
        callback = CustomAsyncIteratorCallbackHandler()
        model = get_ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            callbacks=[callback],
        )
        ## Pass in global variables to implement agent calls
        kb_list = {x["kb_name"]: x for x in get_kb_details()}
        model_container.DATABASE = {name: details['kb_info'] for name, details in kb_list.items()}
        model_container.MODEL = model

        destination_chains = {}

        for p_info in prompt_infos:
            name = p_info["name"]
            prompt_template = p_info["prompt_template"]

            # Category 1: No need to call tools
            if p_info["tools"] == None:
                #Class 1.2 Gauss database problem, need to query the document to solve
                if name == "gauss":
                    docs = search_docs(query, knowledge_base_name, top_k, score_threshold)
                    context = "\n".join([doc.page_content for doc in docs])
                    if "{" in context or "}" in context:
                        context.replace("{","[")
                        context.replace("}","]")
                    prompt_template = prompt_template.replace("{knowledge}", "\" \n" + context +"\n \"")
                    input_msg = History(role="user", content=prompt_template).to_msg_template(False)
                    chat_prompt = CustomKnowledgePromptTemplate(
                        template=prompt_template,
                        input_variables=["input", "history"]
                    )
                    memory = ConversationBufferWindowMemory(k=HISTORY_LEN * 2)
                    for message in history:
                        # Check the role of the message
                        if "Return a markdown code snippet with a JSON object formatted to look like:" in message.content:
                            continue
                        if message.role == 'user':
                            # Add User Message
                            memory.chat_memory.add_user_message(message.content)
                        else:
                            # Add AI Message
                            memory.chat_memory.add_ai_message(message.content)
                    chain = LLMChain(prompt=chat_prompt, llm=model, memory=memory)
                    destination_chains[name] = chain
                #Category 1.1 General database problems, directly solved by relying on large model capabilities
                if name == "general":
                    input_msg = History(role="user", content=prompt_template).to_msg_template(False)
                    chat_prompt = CustomKnowledgePromptTemplate(
                        template=prompt_template,
                        input_variables=["input", "history"]
                    )
                    memory = ConversationBufferWindowMemory(k=HISTORY_LEN * 2)
                    for message in history:
                        # Check the role of the message
                        if "Return a markdown code snippet with a JSON object formatted to look like:" in message.content:
                            continue
                        if message.role == 'user':
                            # Add User Message
                            memory.chat_memory.add_user_message(message.content)
                        else:
                            # Add AI Message
                            memory.chat_memory.add_ai_message(message.content)
                    chain = LLMChain(prompt=chat_prompt, llm=model, memory=memory)
                    destination_chains[name] = chain
                if name == "normal":
                    input_msg = History(role="user", content=prompt_template).to_msg_template(False)
                    chat_prompt = CustomKnowledgePromptTemplate(
                        template=prompt_template,
                        input_variables=["input", "history"]
                    )
                    memory = ConversationBufferWindowMemory(k=HISTORY_LEN * 2)
                    for message in history:
                        # Check the role of the message
                        if "Return a markdown code snippet with a JSON object formatted to look like:" in message.content:
                            continue
                        if message.role == 'user':
                            # Add User Message
                            memory.chat_memory.add_user_message(message.content)
                        else:
                            # Add AI Message
                            memory.chat_memory.add_ai_message(message.content)
                    chain = LLMChain(prompt=chat_prompt, llm=model, memory=memory)
                    destination_chains[name] = chain 
            #Category 2: Need to call tools
            else:               
                prompt = CustomToolPromptTemplate(
                    template=prompt_template,
                    tools=p_info["tools"],
                    input_variables=["input", "intermediate_steps", "history"]
                )
                chain = LLMChain(llm=model, prompt=prompt)
                agent = LLMSingleActionAgent(
                    llm_chain=chain,
                    output_parser=CustomOutputParser(),
                    stop=["\nObservation:", "Observation:", "<|im_end|>"],  # The Qwen model uses this
                    allowed_tools=tool_names,
                )
                # Convert history to agent memory
                memory = ConversationBufferWindowMemory(k=HISTORY_LEN * 2)
                for message in history:
                    # Check the role of the message
                    if message.role == 'user':
                        # Add User Message
                        memory.chat_memory.add_user_message(message.content)
                    else:
                        # Add AI Message
                        memory.chat_memory.add_ai_message(message.content)
                agent_executor = AgentExecutor.from_agent_and_tools(agent=agent,
                                                                    tools=tools,
                                                                    verbose=True,
                                                                    memory=memory,
                                                                    )
                destination_chains[name] = agent_executor
        
        # Construct a ROUTER prompt
        destinations = [f"{p['name']}: {p['description']}" for p in prompt_infos]
        destinations_str = "\n".join(destinations)

        default_prompt = ChatPromptTemplate.from_template("{input}")
        default_chain = LLMChain(llm=model, prompt=default_prompt)

        MULTI_PROMPT_ROUTER_TEMPLATE = get_prompt_template("db_chat", "English")
        router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
            destinations=destinations_str
        )
        router_prompt = PromptTemplate(
            template=router_template,
            input_variables=["input"],
            output_parser=RouterOutputParser(),
        )

        router_chain = LLMRouterChain.from_llm(model, router_prompt)

        # Complete the construction of the entire chain
        chain = MultiPromptChain(router_chain=router_chain,
                                 destination_chains=destination_chains,
                                 default_chain=default_chain, verbose=True
                                 )

        while True:
            try:
                task = asyncio.create_task(wrap_done(
                    chain.acall(query, callbacks=[callback], include_run_info=True),
                    callback.done))
                break
            except:
                pass
        
        router_complete = False
        is_tool = False
        router_result = ""
        if stream:
            async for chunk in callback.aiter():
                tools_use = []
                # Use server-sent-events to stream the response
                data = json.loads(chunk)
                if data["status"] == Status.start or data["status"] == Status.complete:
                    if data["status"] == Status.complete:
                        router_complete = True
                    if data["llm_token"]:
                        yield json.dumps({"answer": data["llm_token"]}, ensure_ascii=False)
                elif data["status"] == Status.error:
                    tools_use.append("\n```\n")
                    tools_use.append("Tool Name: " + data["tool_name"])
                    tools_use.append("Tool Status: " + "Invocation Failure")
                    tools_use.append("Error: " + data["error"])
                    tools_use.append("Retry")
                    tools_use.append("\n```\n")
                    yield json.dumps({"tools": tools_use}, ensure_ascii=False)
                elif data["status"] == Status.tool_finish:
                    tools_use.append("\n```\n")
                    tools_use.append("Tool Name: " + data["tool_name"])
                    tools_use.append("Tool Status: " + "Invocation Successful")
                    tools_use.append("Tool Input: " + data["input_str"])
                    tools_use.append("Tool Output: " + data["output_str"])
                    tools_use.append("\n```\n")
                    yield json.dumps({"tools": tools_use}, ensure_ascii=False)
                elif data["status"] == Status.agent_finish:
                    yield json.dumps({"final_answer": data["final_answer"]}, ensure_ascii=False)
                else:
                    if not router_complete:
                        router_result += data["llm_token"]
                        if "\"destination\": \"management\"" in router_result:
                            is_tool = True
                    if router_complete and not is_tool:
                        yield json.dumps({"final_answer": data["llm_token"]}, ensure_ascii=False)
                    else:
                        yield json.dumps({"answer": data["llm_token"]}, ensure_ascii=False)

        else:
            answer = ""
            final_answer = ""
            async for chunk in callback.aiter():
                # Use server-sent-events to stream the response
                data = json.loads(chunk)
                if data["status"] == Status.start or data["status"] == Status.complete:
                    continue
                if data["status"] == Status.error:
                    answer += "\n```\n"
                    answer += "Tool Name: " + data["tool_name"] + "\n"
                    answer += "Tool Status: " + "Invocation Failure" + "\n"
                    answer += "Error: " + data["error"] + "\n"
                    answer += "\n```\n"
                if data["status"] == Status.tool_finish:
                    answer += "\n```\n"
                    answer += "Tool Name: " + data["tool_name"] + "\n"
                    answer += "Tool Status: " + "Invocation Successful" + "\n"
                    answer += "Tool Input: " + data["input_str"] + "\n"
                    answer += "Tool Output: " + data["output_str"] + "\n"
                    answer += "\n```\n"
                if data["status"] == Status.agent_finish:
                    final_answer = data["final_answer"]
                else:
                    answer += data["llm_token"]

            yield json.dumps({"answer": answer, "final_answer": final_answer}, ensure_ascii=False)

        await task

    return StreamingResponse(db_chat_iterator(query=query,
                                              history=history,
                                              model_name=model_name,
                                              prompt_name=prompt_name),
                             media_type="text/event-stream")
