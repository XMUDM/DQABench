<div align='center'>
    <h1>Evaluation Code of Benchmark</h1>
</div>

<p align='center'>
    【English | <a href="README_zh.md">中文</a>】
</p>

## Contents

* [Overview](#1-overview)
* [Environment Setup](#2-environment-setup)
* [Data Preparation](#3-data-preparation)
* [Download Model](#4-download-model)
* [File Description](#5-file-description)
* [One-click Start](#6-one-click-start)
* [Relevant Projects](#7-relevant-projects)

## 1. Overview

This section is the complete evaluation process of DQA. The evaluation process includes a variety of indicators and a standardized evaluation process to ensure the comprehensiveness, accuracy, and fairness of the evaluation. The evaluation process supports multiple mainstream large language models and can support more models and test indicators through simple extensions. This directory provides the specific implementation and usage of the evaluation process.

## 2. Environment Setup

2.1 First, ensure that your machine has Python 3.9 installed.
```shell
$ python --version
Python 3.9.19
```

2.2 Next, create a virtual environment and install the dependencies for the project within it.
```shell
# Clone the repository
$ git clone ......

# Enter the directory
$ cd DQA-Bench/3_Evaluation_Code_of_Benchmark

# Install all dependencies
$ pip install -r requirements.txt
```

## 3. Data Preparation

Use [Dataset of Benchmark DQA](../1_Dataset_of_Benchmark_DQA/README.md) or create your own dataset. The dataset directory structure is as follows:

```
├─DQABenchmark_en
│  ├─Gauss
│  │  ├─GaussDB
│  │  └─openGauss
│  ├─General
│  │  ├─Select
│  │  └─Forum_QA
│  └─Tool
│      ├─Scenarios-based-QA
│      └─Tool-based-QA
└─DQABenchmark_zh
    ├─Gauss
    │  ├─GaussDB
    │  └─openGauss
    ├─General
    │  ├─Select
    │  └─Forum_QA
    └─Tool
        ├─Scenarios-based-QA
        └─Tool-based-QA
```

Each directory should contain `test.json` and `validation.json`.

## 4. Download Model

If you need to run this project locally or in an offline environment, you first need to download the required model locally. Usually, open source LLM and Embedding models can be downloaded from [HuggingFace](https://huggingface.co/models).

Take the conversation model [baichuan-inc/Baichuan2-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat) and the Embedding model [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) as examples:

To download the model, you need to first [install Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage) and then run

```Shell
$ git lfs install
$ git clone https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat
$ git clone https://huggingface.co/BAAI/bge-large-zh
```

## 5. File Description

- `config.py`

  This file is a prompt and test metric configuration file, which contains customized prompts and metric selection settings for various database questions. If you want to select a test metric, just change the corresponding field to true.

- `customMetric.py`
  
  This file is a specific implementation of a custom test metric written based on the deepeval framework. In WinRateMetric, the path of the base model run result data can be directly specified without repeated generation during the evaluation process, thereby improving efficiency and reducing cost.
  ```shell
  with open('', 'r', encoding='utf-8') as f: # Please fill in the path of the data generated by the base model, such as 'gpt-3.5-turbo-0125_general_zh_cases.json'
      data = json.load(f)
  ```

- `llm.py`
  
  This file is a custom LLM configuration file written based on the deepeval framework. If you use an online model, you need to fill in your api_key in this file; if you use a local model, you need to set the model path to the local absolute path of the model. If you do not change the path, the huggingface model is used by default.

## 6. One-click Start

Start the project with the following commands.
```shell
$ python startup.py \
-model <model_name> \
-benchmark {select|general|gauss|tool} \
-language {en|zh} \
-evaluator <evaluator_name> \
[-action {data|eval}]

# -model      Required  The name of the model being evaluated.
# -benchmark  Required  Select the part of evaluation, which can only be one of select, general, gauss and tool.
# -language   Required  The language of the dataset, can only be one of en and zh.
# -evaluator  Required  Evaluation model name.
# -action     Optional  Select the part of code execution, which can only be one of data and eval. "data" means only generating the corresponding model running result data set, and "eval" means only performing evaluation. If this parameter is not set, all code will be executed by default.
```

## 7. Relevant Projects

https://github.com/confident-ai/deepeval