<div align='center'>
    <h1>Evaluation Code of Benchmark</h1>
</div>

<p align='center'>
    【<a href="README.md">English</a> | 中文】
</p>

## 目录

* [概述](#1-概述)
* [环境配置](#2-环境配置)
* [数据准备](#3-数据准备)
* [模型下载](#4-模型下载)
* [文件说明](#5-文件说明)
* [一键启动](#6-一键启动)
* [相关项目](#7-相关项目)

## 1. 概述

这部分是 DQA 的完整评估流程。评估流程包括多样化的指标和标准化的评估过程，以确保评估的全面性、准确性和公平性。该评估流程支持多个主流的大语言模型，并可以通过简单扩展支持更多模型和测试指标。本目录提供了评估流程的具体实现和使用方法。

## 2. 环境配置

2.1 首先，确保你的机器安装了 Python 3.9
```shell
$ python --version
Python 3.9.19
```

2.2 接着，创建一个虚拟环境，并在虚拟环境内安装项目的依赖
```shell
# 拉取仓库
$ git clone ......

# 进入目录
$ cd DQA-Bench/3_Evaluation_Code_of_Benchmark

# 安装全部依赖
$ pip install -r requirements.txt
```

## 3. 数据准备

使用 [Dataset of Benchmark DQA](../1_Dataset_of_Benchmark_DQA/README.md) 或自行创建数据集，数据集目录结构如下：

```
├─DQABenchmark_en
│  ├─Gauss
│  │  ├─GaussDB
│  │  └─openGauss
│  ├─General
│  │  ├─Select
│  │  └─Forum_QA
│  └─Tool
│      ├─Scenarios-based-QA
│      └─Tool-based-QA
└─DQABenchmark_zh
    ├─Gauss
    │  ├─GaussDB
    │  └─openGauss
    ├─General
    │  ├─Select
    │  └─Forum_QA
    └─Tool
        ├─Scenarios-based-QA
        └─Tool-based-QA
```

每个目录需包含 `test.json` 和 `validation.json` 文件。

## 4. 模型下载

如果您需要在本地或离线环境中运行此项目，您首先需要将所需的模型下载到本地，通常开源 LLM 与 Embedding 模型可以从 [HuggingFace](https://huggingface.co/models) 下载。

以对话模型[baichuan-inc/Baichuan2-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat) 与 Embedding 模型 [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) 为例：

下载模型需要先[安装 Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)，然后运行

```Shell
$ git lfs install
$ git clone https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat
$ git clone https://huggingface.co/BAAI/bge-large-zh
```

## 5. 文件说明

- `config.py`

  该文件是 prompt 和测试指标配置文件，包含对于数据库各类问题的定制化 prompt 以及指标选择设置，若想要选择某个测试指标，只需将对应字段改为 true 即可。

- `customMetric.py` 
  
  该文件是基于 deepeval 框架编写的自定义测试指标的具体实现。其中，WinRateMetric 中可以直接指定 base model 运行结果数据的路径，而无需在评估过程中重复生成，从而提高效率并减少开销。
  ```shell
  with open('', 'r', encoding='utf-8') as f: # Please fill in the path of the data generated by the base model, such as 'gpt-3.5-turbo-0125_general_zh_cases.json'
      data = json.load(f)
  ```

- `llm.py`
  
  该文件是基于 deepeval 框架编写的自定义 LLM 配置文件。若采用在线模型，需在该文件中填写您的api_key；若采用本地模型，需将模型路径设置为模型在本地的绝对路径，若不更改路径，则默认使用 huggingface 模型。

## 6. 一键启动

按照以下命令启动项目
```shell
$ python startup.py \
-model <model_name> \
-benchmark {select|general|gauss|tool} \
-language {en|zh} \
-evaluator <evaluator_name> \
[-action {data|eval}]

# -model      必需  被评估模型名称
# -benchmark  必需  选择评估部分，只能是 select, general, gauss 和 tool 之一
# -language   必需  数据集语言，只能是 en 和 zh 之一
# -evaluator  必需  评估模型名称
# -action     可选  选择代码执行部分，只能是 data 和 eval 之一，data 表示只生成对应的模型运行结果数据集，eval 表示只进行评估。若不设置该参数，则默认全部执行
```

## 7. 相关项目

https://github.com/confident-ai/deepeval